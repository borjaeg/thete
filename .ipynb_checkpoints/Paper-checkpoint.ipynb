{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import collections\n",
    "import os\n",
    "from time import time\n",
    "from PIL import Image\n",
    "\n",
    "# Data Science\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Natural Language processing\n",
    "import nltk\n",
    "\n",
    "# Algorithms / estimators\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Feature Extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Process\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "# Corpus\n",
    "from documentModel import DocumentModel as DM\n",
    "from export_results import *\n",
    "\n",
    "def save_image(image, url='../images/', name = 'default'):\n",
    "    image.savefig(url + name)\n",
    "    Image.open(url + name + '.png').convert('L').save(url + name + '.png')\n",
    "    \n",
    "def plot_image(x, y, title=\"title\", ylim = [0, 1.02], xlim = [2, 50.5], \n",
    "               colors=\"rgbmyc\", models=None, name=\"name\", labels=[], ylabel = \"ylabel\", loc=\"better\", markers=\".,ov<>\"):\n",
    "    plt.figure(figsize=(14,13))\n",
    "    plt.ylim(ylim)\n",
    "    plt.xlim(xlim)\n",
    "    plt.xlabel(\"Misclassification Cost Ratio\")\n",
    "    plt.ylabel(ylabel)\n",
    "    \n",
    "    plt.style.use('paper.mplstyle')\n",
    "    \n",
    "    for model, marker in zip(models, markers):\n",
    "        plt.plot(x, y[model[0]])\n",
    "        plt.scatter(x, y[model[0]], marker=marker, s=300, label=model[0])\n",
    "        #plt.fill_between(costs, np.asarray(recall_avg[model[0]]) - recall_std[model[0]], \n",
    "        #             np.asarray(recall_avg[model[0]]) + recall_std[model[0]], \n",
    "        #             alpha=0.1, color=color)\n",
    "    \n",
    "    plt.xticks(x, labels, rotation='vertical')\n",
    "    plt.legend(loc=loc, prop={'size':30})\n",
    "\n",
    "    save_image(plt,'../images/', name)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def normalize(recall_avg, precision_avg):\n",
    "    recall_avg_normalized = {}\n",
    "\n",
    "    for key, value in recall_avg.items():\n",
    "        recall_avg_normalized[key] = []\n",
    "\n",
    "    for key, values in recall_avg.items():\n",
    "        for value in values:\n",
    "            recall_avg_normalized[key].append(float(value * 100))\n",
    " \n",
    "\n",
    "    precision_avg_normalized = {}\n",
    "\n",
    "    for key, value in precision_avg.items():\n",
    "        precision_avg_normalized[key] = []\n",
    "\n",
    "    for key, values in precision_avg.items():\n",
    "        for value in values:\n",
    "            precision_avg_normalized[key].append(float(value * 100))    \n",
    "    return recall_avg_normalized, precision_avg_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Rule Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming annotated files into training datasets...\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "stop_words = ['a', 'bajo', 'en', 'para','un', 'la', 'el', 'los', 'las', 'su', 'sus', 'través', 'al','con', \\\n",
    "             'más', 'muy', 'cual', 'poco', 'que']\n",
    "\n",
    "print(\"Transforming annotated files into training datasets...\")\n",
    "dm = DM()\n",
    "fito_dataset = dm.get_sentences(0)\n",
    "X, y = fito_dataset[\"data\"], fito_dataset[\"target\"]\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rule_data = pd.DataFrame({\"rule\": X, \"tag\": y})\n",
    "permissions = rule_data[rule_data[\"tag\"] == 0]\n",
    "prohibitions = rule_data[rule_data[\"tag\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rule</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aplicar sobre el suelo en pequeños montones o ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aplicar en pulverización normal, variando la d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>En la etiqueta deberán figurar las instruccion...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Aplicar en pulverización normal, dependiendo d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Aplicar en pulverización normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 rule  tag\n",
       "0   Aplicar sobre el suelo en pequeños montones o ...    0\n",
       "3   Aplicar en pulverización normal, variando la d...    0\n",
       "4   En la etiqueta deberán figurar las instruccion...    0\n",
       "9   Aplicar en pulverización normal, dependiendo d...    0\n",
       "14                    Aplicar en pulverización normal    0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permissions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rule</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Evitar que el producto caiga sobre las plantas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Advertir en la etiqueta que es peligroso para ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>No mezclar con aceites ni productos de reacció...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>No aplicar aceites minerales durante los 21 dí...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>No aplicar en cultivos cuyos frutos sean desti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                rule  tag\n",
       "1  Evitar que el producto caiga sobre las plantas...    1\n",
       "2  Advertir en la etiqueta que es peligroso para ...    1\n",
       "5  No mezclar con aceites ni productos de reacció...    1\n",
       "6  No aplicar aceites minerales durante los 21 dí...    1\n",
       "7  No aplicar en cultivos cuyos frutos sean desti...    1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prohibitions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = [\"1:1\",\"1:2\",\"1:4\",\"1:6\",\"1:10\",\"1:25\", \"1:50\", \"1:100\", \"1:1000\", \"1:10000\", \"1:100000\"]\n",
    "# like the ShuffleSplit strategy, stratified random splits do not guarantee \n",
    "#that all folds will be different, although this is still very likely for sizeable datasets\n",
    "#sss = StratifiedShuffleSplit(y, n_iter = 5, test_size=0.2, random_state=0)\n",
    "sss = StratifiedKFold(y, n_folds = 10, random_state = 2016)\n",
    "costs = np.array([2, 3, 4, 6, 10, 25, 50, 100, 1000, 10000, 1000000])\n",
    "axis_costs = np.arange(1,12,1)\n",
    "cxlim = [0.8, 11.15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extractor = TfidfVectorizer(use_idf=True, stop_words=stop_words)\n",
    "extractor.fit(X)\n",
    "\n",
    "def init():\n",
    "    recall_avg = {}\n",
    "    recall_std = {}\n",
    "    precision_avg = {}\n",
    "    precision_std = {}\n",
    "    \n",
    "    for model in models:\n",
    "        recall_avg[model[0]] = []\n",
    "        recall_std[model[0]] = []\n",
    "        precision_avg[model[0]] = []\n",
    "        precision_std[model[0]] = []\n",
    "        \n",
    "    return recall_avg, recall_std, precision_avg, precision_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models = [\n",
    "          (\"Naive Bayes\", MultinomialNB),        \n",
    "          (\"Random Forest\", RandomForestClassifier),\n",
    "          (\"Linear SVM\", SVC),\n",
    "          (\"Baseline\", DummyClassifier),\n",
    "          ]\n",
    "\n",
    "recall_avg, recall_std, precision_avg, precision_std = init()\n",
    "\n",
    "for estimator in models:\n",
    "    for cost in costs:\n",
    "        if estimator[0] == \"Naive Bayes\":\n",
    "            model = estimator[1](class_prior=[1/cost, (cost-1)/cost], fit_prior= False)\n",
    "        elif estimator[0] == \"Random Forest\":\n",
    "            model = estimator[1](class_weight={1:cost-1}, n_estimators = 20)\n",
    "        elif estimator[0] == \"Linear SVM\":\n",
    "            model = estimator[1](class_weight={1:cost-1}, kernel = \"linear\", C = 0.1)\n",
    "        elif estimator[0] == \"Baseline\":\n",
    "            model = estimator[1](strategy = \"constant\", random_state = 2016, constant=1)\n",
    "        else:\n",
    "            model = estimator[1](class_weight={1:cost-1})\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        for train_index, test_index in sss:\n",
    "            X_train, X_test = extractor.transform(X)[train_index], extractor.transform(X)[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            model.fit(X_train, y_train)\n",
    "            precisions.append(precision_score(y_test, model.predict(X_test), pos_label=1))\n",
    "            recalls.append(recall_score(y_test, model.predict(X_test), pos_label=1))\n",
    "        recall_avg[estimator[0]].append(np.average(recalls))\n",
    "        recall_std[estimator[0]].append(np.std(recalls))\n",
    "        precision_avg[estimator[0]].append(np.average(precisions))\n",
    "        precision_std[estimator[0]].append(np.std(precisions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recall_avg_normalized, precision_avg_normalized = normalize(recall_avg, precision_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_image(axis_costs, recall_avg_normalized, title=\"RECALL\", ylim = [80., 100.5], \n",
    "           xlim = cxlim, models=models, name=\"recall\", labels=labels, ylabel=\"Recall (%)\", \n",
    "           loc='lower righ', markers=\"<Do|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_image(axis_costs, precision_avg_normalized, title=\"PRECISION\", ylim = [0, 100], \n",
    "           xlim = cxlim, colors=\"rgbmyc\", models=models, name=\"precision\", labels = labels, ylabel=\"Precision (%)\",\n",
    "           loc =\"lower right\", markers=\"<Do|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recall_baseline = recall_avg['Naive Bayes']\n",
    "precision_baseline = precision_avg['Naive Bayes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = SnowballStemmer(language=\"spanish\")\n",
    "    \n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.stem(t) for t in word_tokenize(doc)]\n",
    "    \n",
    "def init(extractors, models):\n",
    "    recall_avg = {}\n",
    "    recall_std = {}\n",
    "    precision_avg = {}\n",
    "    precision_std = {}\n",
    "    vocabulary_sizes = {}\n",
    "    for model in models:\n",
    "        for extractor in extractors:\n",
    "            name = extractor[0]\n",
    "            recall_avg[name] = []\n",
    "            recall_std[name] = []\n",
    "            precision_avg[name] = []\n",
    "            precision_std[name] = []\n",
    "            vocabulary_sizes[extractor[0]] = []\n",
    "        \n",
    "    return recall_avg, recall_std, precision_avg, precision_std, vocabulary_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extractors = [(\"Stemming\", TfidfVectorizer(tokenizer=LemmaTokenizer(), use_idf=True, stop_words=stop_words))]\n",
    "\n",
    "recall_avg, recall_std, precision_avg, \\\n",
    "    precision_std, vocabulary_sizes = init(extractors, models)\n",
    "    \n",
    "vocabularies = {}    \n",
    "\n",
    "for i, extractor in enumerate(extractors):\n",
    "    extractor[1].fit(X)\n",
    "    print(extractor[0] + \" \" + str(len(extractor[1].vocabulary_)))\n",
    "    vocabularies[extractor[0]] = extractor[1].vocabulary_\n",
    "    for cost in costs:\n",
    "        model = MultinomialNB(class_prior=[1/cost, (cost-1)/cost], fit_prior= False)\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        for train_index, test_index in sss:\n",
    "            X_train, X_test = extractor[1].transform(X)[train_index], extractor[1].transform(X)[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            model.fit(X_train, y_train)\n",
    "            precisions.append(precision_score(y_test, model.predict(X_test), pos_label=1))\n",
    "            recalls.append(recall_score(y_test, model.predict(X_test), pos_label=1))\n",
    "        recall_avg[extractor[0]].append(np.average(recalls))\n",
    "        recall_std[extractor[0]].append(np.std(recalls))\n",
    "        precision_avg[extractor[0]].append(np.average(precisions))\n",
    "        precision_std[extractor[0]].append(np.std(precisions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recall_avg['Naive Bayes'] = recall_baseline\n",
    "precision_avg['Naive Bayes'] = precision_baseline\n",
    "extractors.append((\"Naive Bayes\", MultinomialNB))\n",
    "extractors = extractors[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recall_avg_normalized, precision_avg_normalized = normalize(recall_avg, precision_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_image(axis_costs, recall_avg_normalized, title=\"RECALL\", ylim = [90, 100.5], \n",
    "           xlim = cxlim, colors=\"rg\", models=extractors, name=\"recall_stemming\", labels = labels, ylabel=\"Recall\",\n",
    "        loc =\"lower right\", markers=\"<o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_image(axis_costs, precision_avg_normalized, title=\"PRECISION\", ylim = [30, 80], \n",
    "           xlim = cxlim, colors=\"rg\", models=extractors, name=\"precision_stemming\", labels = labels, ylabel=\"Precision\",\n",
    "           loc =\"lower left\", markers=\"<o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extractors = [(\"n-grams(1..2)\", TfidfVectorizer(ngram_range=(1, 2), use_idf=True, stop_words=stop_words)),\n",
    "             (\"n-grams(2..2)\", TfidfVectorizer(ngram_range=(2, 2), use_idf=True, stop_words=stop_words)),\n",
    "             ]\n",
    "\n",
    "recall_avg, recall_std, precision_avg, \\\n",
    "    precision_std, vocabulary_sizes = init(extractors, models)\n",
    "\n",
    "for i, extractor in enumerate(extractors):\n",
    "    extractor[1].fit(X)\n",
    "    print(extractor[0] + \" \" + str(len(extractor[1].vocabulary_)))\n",
    "    vocabulary_sizes[extractor[0]] = len(extractor[1].vocabulary_)\n",
    "    for cost in costs:\n",
    "        model = MultinomialNB(class_prior=[1/cost, (cost-1)/cost], fit_prior= False)\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        for train_index, test_index in sss:\n",
    "            X_train, X_test = extractor[1].transform(X)[train_index], extractor[1].transform(X)[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            model.fit(X_train, y_train)\n",
    "            precisions.append(precision_score(y_test, model.predict(X_test), pos_label=1))\n",
    "            recalls.append(recall_score(y_test, model.predict(X_test), pos_label=1))\n",
    "        recall_avg[extractor[0]].append(np.average(recalls))\n",
    "        recall_std[extractor[0]].append(np.std(recalls))\n",
    "        precision_avg[extractor[0]].append(np.average(precisions))\n",
    "        precision_std[extractor[0]].append(np.std(precisions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recall_avg['Naive Bayes'] = recall_baseline\n",
    "precision_avg['Naive Bayes'] = precision_baseline\n",
    "extractors.append((\"Naive Bayes\", MultinomialNB))\n",
    "extractors = extractors[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recall_avg_normalized, precision_avg_normalized = normalize(recall_avg, precision_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_image(axis_costs, recall_avg_normalized, title=\"RECALL\", ylim = [92.5, 100.5], \n",
    "           xlim = cxlim, colors=\"rgb\", models=extractors, name=\"recall_n-gram\", labels = labels, ylabel=\"Recall\",\n",
    "           loc=\"lower right\", markers=\"<oD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_image(axis_costs, precision_avg_normalized, title=\"PRECISION\", ylim = [28, 78], \n",
    "           xlim = cxlim, colors=\"rgb\", models=extractors, name=\"precision_n-gram\", labels = labels, ylabel=\"Precision\",\n",
    "           loc=\"lower left\", markers=\"<oD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
