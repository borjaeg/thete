{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import collections\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "# Data Science\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Natural Language processing\n",
    "import nltk\n",
    "\n",
    "## Stemming\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Learning Algorithms / estimators\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import make_scorer, confusion_matrix\n",
    "\n",
    "# Feature Extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Process\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Corpus\n",
    "from documentModel import DocumentModel as DM\n",
    "from export_results import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Process\n",
    "def runInParallel(fns):\n",
    "  proc = []\n",
    "  for fn in fns:\n",
    "    p = Process(target=fn)\n",
    "    p.start()\n",
    "    proc.append(p)\n",
    "  for p in proc:\n",
    "    p.join()\n",
    "    \n",
    "def total_cost_ratio(ground_truth = None, predictions = None, price = 1, expensive_class=1):\n",
    "    \n",
    "    n_class_to_remove = len(ground_truth[ground_truth==expensive_class])\n",
    "    cm = confusion_matrix(ground_truth, predictions)\n",
    "    \n",
    "    if expensive_class == 1:\n",
    "        n_fp = cm[0][1]\n",
    "        n_fn = cm[1][0]\n",
    "    elif expensive_class == 0:\n",
    "        n_fp = cm[1][0]\n",
    "        n_fn = cm[0][1]\n",
    "    \n",
    "    if n_fn != 0 or n_fp != 0:\n",
    "        result = n_class_to_remove/(price * n_fn + n_fp) \n",
    "    else:\n",
    "        result = 100.0\n",
    "\n",
    "    return result\n",
    "    \n",
    "def normalize(recall_avg, precision_avg):\n",
    "    recall_avg_normalized = {}\n",
    "\n",
    "    for key, value in recall_avg.items():\n",
    "        recall_avg_normalized[key] = []\n",
    "\n",
    "    for key, values in recall_avg.items():\n",
    "        for value in values:\n",
    "            recall_avg_normalized[key].append(float(value * 100))\n",
    " \n",
    "\n",
    "    precision_avg_normalized = {}\n",
    "\n",
    "    for key, value in precision_avg.items():\n",
    "        precision_avg_normalized[key] = []\n",
    "\n",
    "    for key, values in precision_avg.items():\n",
    "        for value in values:\n",
    "            precision_avg_normalized[key].append(float(value * 100)) \n",
    "            \n",
    "    return recall_avg_normalized, precision_avg_normalized\n",
    "\n",
    "def write_results(n_experiment, nlp, algorithm, cost_ratio, precision, recall):\n",
    "    import pymysql.cursors\n",
    "\n",
    "    connection = pymysql.connect(host='localhost',\n",
    "                             user='root',\n",
    "                             password='',\n",
    "                             db='agriculture_experiments',\n",
    "                             charset='utf8')\n",
    "\n",
    "    try:\n",
    "        with connection.cursor() as cursor:\n",
    "            sql = \"INSERT INTO test_results(n_experiment, nlp, algorithm, cost_ratio, prec, recall) \\\n",
    "            VALUES(%s, %s, %s, %s, %s, %s)\"\n",
    "            cursor.execute(sql, (n_experiment, nlp, algorithm, cost_ratio, str(precision), str(recall)))\n",
    "            connection.commit()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        connection.close()\n",
    "        \n",
    "def reset_results():\n",
    "    import pymysql.cursors\n",
    "\n",
    "    connection = pymysql.connect(host='localhost',\n",
    "                             user='root',\n",
    "                             password='',\n",
    "                             db='agriculture_experiments',\n",
    "                             charset='utf8')\n",
    "\n",
    "    try:\n",
    "        with connection.cursor() as cursor:\n",
    "            sql = \"TRUNCATE TABLE test_results;\"\n",
    "            cursor.execute(sql)\n",
    "            connection.commit()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = SnowballStemmer(language=\"spanish\")\n",
    "    \n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.stem(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init():\n",
    "    pipelines = []\n",
    "\n",
    "    for element in itertools.product(estimators, nlp):\n",
    "        name = element[1][0] + '-' + element[0][0]\n",
    "        pipeline = Pipeline([('nlp', element[1][1]), ('clf', element[0][1])])\n",
    "        pipelines.append((name , pipeline))\n",
    "        \n",
    "    return pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Rule Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming annotated files into training datasets...\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "stop_words = ['a', 'bajo', 'en', 'para','un', 'la', 'el', 'los', 'las', 'su', 'sus', 'través', 'al','con', \\\n",
    "             'más', 'muy', 'cual', 'poco', 'que']\n",
    "\n",
    "print(\"Transforming annotated files into training datasets...\")\n",
    "dm = DM()\n",
    "fito_dataset = dm.get_sentences(0)\n",
    "X, y = fito_dataset[\"data\"], fito_dataset[\"target\"]\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = [\"1:1\",\"1:2\",\"1:4\",\"1:6\",\"1:10\",\"1:25\", \"1:50\", r\"1:$10^2$\", r\"1:$10^3$\", r\"1:$10^6$\"]\n",
    "# like the ShuffleSplit strategy, stratified random splits do not guarantee \n",
    "#that all folds will be different, although this is still very likely for sizeable datasets\n",
    "\n",
    "costs = np.array([2, 3, 4, 6, 10, 25, 50, 100, 1000, 1000000])\n",
    "axis_costs = np.arange(1,11,1)\n",
    "cxlim = [0.8, 10.15]\n",
    "estimators = [(\"Naive Bayes\", MultinomialNB(fit_prior=False)), \n",
    "              (\"Random Forest\", RandomForestClassifier(n_estimators=20, n_jobs=2)), \n",
    "              (\"SVM\", SVC(kernel='linear', C = 0.1)),\n",
    "              (\"Logistic\", LogisticRegression()),\n",
    "              (\"LDA\", LDA()),\n",
    "              (\"Perceptron\", LogisticRegression()),\n",
    "              (\"Baseline\", DummyClassifier(strategy = \"constant\", constant=1))]\n",
    "\n",
    "nlp = [(\"None\", TfidfVectorizer(use_idf = True, stop_words=stop_words)),\n",
    "       (\"Stemming\", TfidfVectorizer(use_idf = True, stop_words=stop_words, tokenizer=LemmaTokenizer())), \n",
    "       (\"Bigrams\", TfidfVectorizer(use_idf = True, stop_words=stop_words, ngram_range=(2, 2))), \n",
    "       (\"Combination\", TfidfVectorizer(use_idf = True, stop_words=stop_words, ngram_range=(1, 2)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Experiment Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def experiment(pattern = \".*\"):\n",
    "    \n",
    "    import random\n",
    "    \n",
    "    t0 = time.time()\n",
    "    pipes = init()\n",
    "    \n",
    "    random_state = random.randint(0,100)\n",
    "    sss = StratifiedKFold(y, n_folds = 10, shuffle = True, random_state = random_state)\n",
    "    for estimator in pipes:\n",
    "        name = estimator[0]\n",
    "        extractor = estimator[1].steps[0][1].fit(X)\n",
    "        if re.match(pattern, name) is not None:\n",
    "            print(\"Trying: \" + name + \" ...\")\n",
    "            for cost in costs:\n",
    "                if \"Naive Bayes\" in name:\n",
    "                    model = estimator[1].set_params(clf__class_prior=[1/cost, (cost-1)/cost]).steps[1][1]\n",
    "                elif \"Random Forest\" in name:\n",
    "                    model = estimator[1].set_params(clf__class_weight={1:cost-1}).steps[1][1]\n",
    "                elif \"SVM\" in name:\n",
    "                    model = estimator[1].set_params(clf__class_weight={1:cost-1}).steps[1][1]\n",
    "                elif \"Logistic\" in name:\n",
    "                    model = estimator[1].set_params(clf__class_weight={1:cost-1}).steps[1][1]\n",
    "                elif \"LDA\" in name:\n",
    "                    model = estimator[1].set_params(clf__priors=[1/cost, (cost-1)/cost]).steps[1][1]\n",
    "                elif \"Perceptron\" in name:\n",
    "                    model = estimator[1].set_params(clf__class_weight={1:cost-1}).steps[1][1]\n",
    "                elif \"Baseline\" in name:\n",
    "                    model = estimator[1].steps[1][1]\n",
    "            \n",
    "                precisions = []\n",
    "                recalls = []\n",
    "                for train_index, test_index in sss:\n",
    "                    X_train, X_test = \\\n",
    "                            extractor.transform(X)[train_index], extractor.transform(X)[test_index]\n",
    "                    y_train, y_test = y[train_index], y[test_index]\n",
    "                    model.fit(X_train, y_train)\n",
    "                    precisions.append(precision_score(y_test, model.predict(X_test), pos_label=1))\n",
    "                    recalls.append(recall_score(y_test, model.predict(X_test), pos_label=1))\n",
    "                    \n",
    "                write_results(str(random_state), name.split(\"-\")[0], name.split(\"-\")[1], \\\n",
    "                                  str(cost), np.mean(np.array(precisions)), \\\n",
    "                                  np.mean(np.array(recalls)))\n",
    "                \n",
    "    t1 = time.time()\n",
    "    print()\n",
    "    print(\"Execution time: %.3f min\" % ((t1 - t0)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying: None-Perceptron ...\n",
      "Trying: Stemming-Perceptron ...\n",
      "Trying: Bigrams-Perceptron ...\n",
      "Trying: Combination-Perceptron ...\n",
      "\n",
      "Execution time: 2.282 min\n",
      "Trying: None-Perceptron ...\n",
      "Trying: Stemming-Perceptron ...\n",
      "Trying: Bigrams-Perceptron ...\n",
      "Trying: Combination-Perceptron ...\n",
      "\n",
      "Execution time: 2.234 min\n",
      "Trying: None-Perceptron ...\n",
      "Trying: Stemming-Perceptron ...\n",
      "Trying: Bigrams-Perceptron ...\n",
      "Trying: Combination-Perceptron ...\n",
      "\n",
      "Execution time: 2.241 min\n",
      "Trying: None-Perceptron ...\n",
      "Trying: Stemming-Perceptron ...\n",
      "Trying: Bigrams-Perceptron ...\n",
      "Trying: Combination-Perceptron ...\n",
      "\n",
      "Execution time: 2.264 min\n",
      "Trying: None-Perceptron ...\n",
      "Trying: Stemming-Perceptron ...\n",
      "Trying: Bigrams-Perceptron ...\n",
      "Trying: Combination-Perceptron ...\n",
      "\n",
      "Execution time: 2.248 min\n",
      "Trying: None-Perceptron ...\n",
      "Trying: Stemming-Perceptron ...\n",
      "Trying: Bigrams-Perceptron ...\n",
      "Trying: Combination-Perceptron ...\n",
      "\n",
      "Execution time: 2.270 min\n",
      "Trying: None-Perceptron ...\n",
      "Trying: Stemming-Perceptron ...\n",
      "Trying: Bigrams-Perceptron ...\n",
      "Trying: Combination-Perceptron ...\n",
      "\n",
      "Execution time: 2.228 min\n",
      "Trying: None-Perceptron ...\n",
      "Trying: Stemming-Perceptron ...\n",
      "Trying: Bigrams-Perceptron ...\n",
      "Trying: Combination-Perceptron ...\n",
      "\n",
      "Execution time: 2.259 min\n",
      "Trying: None-Perceptron ...\n",
      "Trying: Stemming-Perceptron ...\n",
      "Trying: Bigrams-Perceptron ...\n",
      "Trying: Combination-Perceptron ...\n",
      "\n",
      "Execution time: 2.258 min\n"
     ]
    }
   ],
   "source": [
    "#reset_results()\n",
    "#runInParallel([experiment] * 10)\n",
    "for i in range(9):\n",
    "    experiment(pattern=\".*Perceptron.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
