{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import collections\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "# Data Science\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Natural Language processing\n",
    "import nltk\n",
    "\n",
    "## Stemming\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Learning Algorithms / estimators\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import make_scorer, confusion_matrix\n",
    "\n",
    "# Feature Extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Process\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Plotting\n",
    "from matplotlib.markers import MarkerStyle\n",
    "from PIL import Image\n",
    "\n",
    "# Corpus\n",
    "from documentModel import DocumentModel as DM\n",
    "from export_results import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Process\n",
    "def runInParallel(fns):\n",
    "  proc = []\n",
    "  for fn in fns:\n",
    "    p = Process(target=fn)\n",
    "    p.start()\n",
    "    proc.append(p)\n",
    "  for p in proc:\n",
    "    p.join()\n",
    "    \n",
    "def total_cost_ratio(ground_truth = None, predictions = None, price = 1, expensive_class=1):\n",
    "    \n",
    "    n_class_to_remove = len(ground_truth[ground_truth==expensive_class])\n",
    "    cm = confusion_matrix(ground_truth, predictions)\n",
    "    \n",
    "    if expensive_class == 1:\n",
    "        n_fp = cm[0][1]\n",
    "        n_fn = cm[1][0]\n",
    "    elif expensive_class == 0:\n",
    "        n_fp = cm[1][0]\n",
    "        n_fn = cm[0][1]\n",
    "    \n",
    "    if n_fn != 0 or n_fp != 0:\n",
    "        result = n_class_to_remove/(price * n_fn + n_fp) \n",
    "    else:\n",
    "        result = 100.0\n",
    "\n",
    "    return result\n",
    "    \n",
    "def normalize(recall_avg, precision_avg):\n",
    "    recall_avg_normalized = {}\n",
    "\n",
    "    for key, value in recall_avg.items():\n",
    "        recall_avg_normalized[key] = []\n",
    "\n",
    "    for key, values in recall_avg.items():\n",
    "        for value in values:\n",
    "            recall_avg_normalized[key].append(float(value * 100))\n",
    " \n",
    "\n",
    "    precision_avg_normalized = {}\n",
    "\n",
    "    for key, value in precision_avg.items():\n",
    "        precision_avg_normalized[key] = []\n",
    "\n",
    "    for key, values in precision_avg.items():\n",
    "        for value in values:\n",
    "            precision_avg_normalized[key].append(float(value * 100)) \n",
    "            \n",
    "    return recall_avg_normalized, precision_avg_normalized\n",
    "\n",
    "def write_results(n_experiment, nlp, algorithm, cost_ratio, precision, recall):\n",
    "    import pymysql.cursors\n",
    "\n",
    "    connection = pymysql.connect(host='localhost',\n",
    "                             user='root',\n",
    "                             password='',\n",
    "                             db='agriculture_experiments',\n",
    "                             charset='utf8')\n",
    "\n",
    "    try:\n",
    "        with connection.cursor() as cursor:\n",
    "            sql = \"INSERT INTO results(n_experiment, nlp, algorithm, cost_ratio, prec, recall) \\\n",
    "            VALUES(%s, %s, %s, %s, %s, %s)\"\n",
    "            cursor.execute(sql, (n_experiment, nlp, algorithm, cost_ratio, str(precision), str(recall)))\n",
    "            connection.commit()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        connection.close()\n",
    "        \n",
    "def reset_results():\n",
    "    import pymysql.cursors\n",
    "\n",
    "    connection = pymysql.connect(host='localhost',\n",
    "                             user='root',\n",
    "                             password='',\n",
    "                             db='agriculture_experiments',\n",
    "                             charset='utf8')\n",
    "\n",
    "    try:\n",
    "        with connection.cursor() as cursor:\n",
    "            sql = \"TRUNCATE TABLE results;\"\n",
    "            cursor.execute(sql)\n",
    "            connection.commit()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = SnowballStemmer(language=\"spanish\")\n",
    "    \n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.stem(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init():\n",
    "    pipelines = []\n",
    "\n",
    "    for element in itertools.product(estimators, nlp):\n",
    "        name = element[1][0] + '-' + element[0][0]\n",
    "        pipeline = Pipeline([('nlp', element[1][1]), ('clf', element[0][1])])\n",
    "        pipelines.append((name , pipeline))\n",
    "        \n",
    "    return pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Rule Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming annotated files into training datasets...\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "stop_words = ['a', 'bajo', 'en', 'para','un', 'la', 'el', 'los', 'las', 'su', 'sus', 'través', 'al','con', \\\n",
    "             'más', 'muy', 'cual', 'poco', 'que']\n",
    "\n",
    "print(\"Transforming annotated files into training datasets...\")\n",
    "dm = DM()\n",
    "fito_dataset = dm.get_sentences(0)\n",
    "X, y = fito_dataset[\"data\"], fito_dataset[\"target\"]\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rule_data = pd.DataFrame({\"rule\": X, \"tag\": y})\n",
    "permissions = rule_data[rule_data[\"tag\"] == 0]\n",
    "prohibitions = rule_data[rule_data[\"tag\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rule</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aplicar sobre el suelo en pequeños montones o ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aplicar en pulverización normal, variando la d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>En la etiqueta deberán figurar las instruccion...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Aplicar en pulverización normal, dependiendo d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Aplicar en pulverización normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 rule  tag\n",
       "0   Aplicar sobre el suelo en pequeños montones o ...    0\n",
       "3   Aplicar en pulverización normal, variando la d...    0\n",
       "4   En la etiqueta deberán figurar las instruccion...    0\n",
       "9   Aplicar en pulverización normal, dependiendo d...    0\n",
       "14                    Aplicar en pulverización normal    0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permissions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rule</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Evitar que el producto caiga sobre las plantas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Advertir en la etiqueta que es peligroso para ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>No mezclar con aceites ni productos de reacció...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>No aplicar aceites minerales durante los 21 dí...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>No aplicar en cultivos cuyos frutos sean desti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                rule  tag\n",
       "1  Evitar que el producto caiga sobre las plantas...    1\n",
       "2  Advertir en la etiqueta que es peligroso para ...    1\n",
       "5  No mezclar con aceites ni productos de reacció...    1\n",
       "6  No aplicar aceites minerales durante los 21 dí...    1\n",
       "7  No aplicar en cultivos cuyos frutos sean desti...    1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prohibitions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = [\"1:1\",\"1:2\",\"1:4\",\"1:6\",\"1:10\",\"1:25\", \"1:50\", r\"1:$10^2$\", r\"1:$10^3$\", r\"1:$10^6$\"]\n",
    "# like the ShuffleSplit strategy, stratified random splits do not guarantee \n",
    "#that all folds will be different, although this is still very likely for sizeable datasets\n",
    "\n",
    "costs = np.array([2, 3, 4, 6, 10, 25, 50, 100, 1000, 1000000])\n",
    "axis_costs = np.arange(1,11,1)\n",
    "cxlim = [0.8, 10.15]\n",
    "estimators = [(\"Naive Bayes\", MultinomialNB(fit_prior=False)), \n",
    "              (\"Random Forest\", RandomForestClassifier(n_estimators=20)), \n",
    "              (\"SVM\", SVC(kernel='linear', C = 0.1)),\n",
    "              (\"Baseline\", DummyClassifier(strategy = \"constant\", constant=1))]\n",
    "\n",
    "nlp = [(\"None\", TfidfVectorizer(use_idf = True, stop_words=stop_words)),\n",
    "       (\"Stemming\", TfidfVectorizer(use_idf = True, stop_words=stop_words, tokenizer=LemmaTokenizer())), \n",
    "       (\"Bigrams\", TfidfVectorizer(use_idf = True, stop_words=stop_words, ngram_range=(2, 2))), \n",
    "       (\"Combination\", TfidfVectorizer(use_idf = True, stop_words=stop_words, ngram_range=(1, 2)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Experiment Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def experiment(pattern = \".*\"):\n",
    "    \n",
    "    import random\n",
    "    \n",
    "    t0 = time.time()\n",
    "    pipes = init()\n",
    "    \n",
    "    random_state = random.randint(0,100)\n",
    "    sss = StratifiedKFold(y, n_folds = 10, shuffle = True, random_state = random_state)\n",
    "    for estimator in pipes:\n",
    "        name = estimator[0]\n",
    "        extractor = estimator[1].steps[0][1].fit(X)\n",
    "        if re.match(pattern, name) is not None:\n",
    "            print(\"Trying: \" + name + \" ...\")\n",
    "            for cost in costs:\n",
    "                if \"Naive Bayes\" in name:\n",
    "                    model = estimator[1].set_params(clf__class_prior=[1/cost, (cost-1)/cost]).steps[1][1]\n",
    "                elif \"Random Forest\" in name:\n",
    "                    model = estimator[1].set_params(clf__class_weight={1:cost-1}).steps[1][1]\n",
    "                elif \"SVM\" in name:\n",
    "                    model = estimator[1].set_params(clf__class_weight={1:cost-1}).steps[1][1]\n",
    "                elif \"Baseline\" in name:\n",
    "                    model = estimator[1].steps[1][1]\n",
    "            \n",
    "                precisions = []\n",
    "                recalls = []\n",
    "                for train_index, test_index in sss:\n",
    "                    X_train, X_test = \\\n",
    "                            extractor.transform(X)[train_index], extractor.transform(X)[test_index]\n",
    "                    y_train, y_test = y[train_index], y[test_index]\n",
    "                    model.fit(X_train, y_train)\n",
    "                    precisions.append(precision_score(y_test, model.predict(X_test), pos_label=1))\n",
    "                    recalls.append(recall_score(y_test, model.predict(X_test), pos_label=1))\n",
    "                    \n",
    "                write_results(str(random_state), name.split(\"-\")[0], name.split(\"-\")[1], \\\n",
    "                                  str(cost), np.mean(np.array(precisions)), \\\n",
    "                                  np.mean(np.array(recalls)))\n",
    "                \n",
    "    t1 = time.time()\n",
    "    print()\n",
    "    print(\"Execution time: %.3f min\" % ((t1 - t0)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying: None-Naive Bayes ...\n",
      "Trying: None-Naive Bayes ...\n",
      "Trying: None-Naive Bayes ...\n",
      "Trying: None-Naive Bayes ...\n",
      "Trying: None-Naive Bayes ...\n",
      "Trying: None-Naive Bayes ...\n",
      "Trying: None-Naive Bayes ...\n",
      "Trying: None-Naive Bayes ...\n",
      "Trying: None-Naive Bayes ...\n",
      "Trying: None-Naive Bayes ...\n",
      "Trying: Stemming-Naive Bayes ...\n",
      "Trying: Stemming-Naive Bayes ...\n",
      "Trying: Stemming-Naive Bayes ...\n",
      "Trying: Stemming-Naive Bayes ...\n",
      "Trying: Stemming-Naive Bayes ...\n",
      "Trying: Stemming-Naive Bayes ...\n",
      "Trying: Stemming-Naive Bayes ...\n",
      "Trying: Stemming-Naive Bayes ...\n",
      "Trying: Stemming-Naive Bayes ...\n",
      "Trying: Stemming-Naive Bayes ...\n",
      "Trying: Bigrams-Naive Bayes ...\n",
      "Trying: Bigrams-Naive Bayes ...\n",
      "Trying: Bigrams-Naive Bayes ...\n",
      "Trying: Bigrams-Naive Bayes ...\n",
      "Trying: Bigrams-Naive Bayes ...\n",
      "Trying: Bigrams-Naive Bayes ...\n",
      "Trying: Bigrams-Naive Bayes ...\n",
      "Trying: Bigrams-Naive Bayes ...\n",
      "Trying: Bigrams-Naive Bayes ...\n",
      "Trying: Bigrams-Naive Bayes ...\n"
     ]
    }
   ],
   "source": [
    "reset_results()\n",
    "runInParallel([experiment] * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
